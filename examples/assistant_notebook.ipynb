{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import warnings\n",
    "\n",
    "# Filter all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "trained_regressors = {} \n",
    "table_data = []\n",
    "\n",
    "\n",
    "# Loop through models and distributions and evaluate each one\n",
    "for name, values in splitted_data.items():\n",
    "    \n",
    "    X_train_survival = values['X_train_survival']\n",
    "    X_test_survival = values['X_test_survival']\n",
    "    y_train_survival = values['y_train_survival']\n",
    "    y_test_survival = values['y_test_survival']\n",
    "\n",
    "    X_train = values['X_train']\n",
    "    y_train = values['y_train']\n",
    "    X_test = values['X_test']\n",
    "    y_test = values['y_test']\n",
    "\n",
    "    X_train_mn_ext_feat = values['X_train_mn_ext_feat']\n",
    "    X_test_mn_ext_feat = values['X_test_mn_ext_feat']\n",
    "    y_train_mn_ext_feat = values['y_train_mn_ext_feat'] \n",
    "    y_test_mn_ext_feat = values['y_test_mn_ext_feat']\n",
    "    \n",
    "    X_train_tsfel = values['X_train_tsfel']\n",
    "    y_train_tsfel = values['y_train_tsfel']\n",
    "    X_test_tsfel = values['X_test_tsfel']\n",
    "    y_test_tsfel = values['y_test_tsfel']\n",
    "\n",
    "    THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "\n",
    "    results = pd.DataFrame(index=values['y_test'].index)\n",
    "\n",
    "    if X_train_mn_ext_feat.isna().sum().sum() > 0:\n",
    "        X_train_mn_ext_feat = X_train_mn_ext_feat.fillna(0)\n",
    "    if X_test_mn_ext_feat.isna().sum().sum() > 0:\n",
    "        X_test_mn_ext_feat = X_test_mn_ext_feat.fillna(0)\n",
    "\n",
    "    for regression in list_of_models:\n",
    "        for distribution in distributions:\n",
    "            if regression not in [#'Prophet', \n",
    "                                #'Prophet_simple_threshold', \n",
    "                                'Survival_Analysis', \n",
    "                                #'hidden_markov_models', \n",
    "                                'Survival_Analysis_manual_extracted_features',\n",
    "                                'Survival_Analysis_tsfel_extracted_features']:\n",
    "                # Implement the model fitting and evaluation logic here\n",
    "                # print('\\n')\n",
    "                # print('Distribution:', distribution.name)\n",
    "                # print('Model:', str(regression))\n",
    "\n",
    "                regression.fit(X_train, y_train)\n",
    "\n",
    "                trained_regressors[str(regression)+'_'+str(distribution.name)]=[regression]\n",
    "\n",
    "                # getting point forecasts\n",
    "                point_forecasts = regression.predict(X_test)\n",
    "                std = y_train.std()\n",
    "                if distribution == lognorm:\n",
    "                    exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, s=std) for x_ in point_forecasts])\n",
    "                elif distribution == genextreme:\n",
    "                    exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, c = -0.1, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "                elif distribution != lognorm:\n",
    "                    exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "                \n",
    "                #y_test_ = new_df['marked'].loc[y_test.index]\n",
    "                y_test_ = (y_test > THRESHOLD).astype(int)\n",
    "                \n",
    "                y_pred_binary = np.where(exceedance_prob >= 0.5, 1, 0)\n",
    "                results['actual_predictions_from_model'+'_'+str(regression)+'_'+str(distribution.name)]  = point_forecasts\n",
    "                results['y_pred_binary'+'_'+str(regression)+'_'+str(distribution.name)]  = y_pred_binary\n",
    "                trained_regressors[str(regression)+'_'+str(distribution.name)].append(y_pred_binary)\n",
    "\n",
    "                # Compute the confusion matrix\n",
    "                cm = confusion_matrix(y_test_, y_pred_binary)\n",
    "\n",
    "                # Compute the precision, recall, and F1 score\n",
    "                accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "                precision = precision_score(y_test_, y_pred_binary)\n",
    "                recall = recall_score(y_test_, y_pred_binary)\n",
    "                f1 = f1_score(y_test_, y_pred_binary)\n",
    "                roc_auc = roc_auc_score(y_test_, exceedance_prob)\n",
    "\n",
    "                # Print the results\n",
    "                # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "                # print(f\"Accuracy: {accuracy:.2f}\")\n",
    "                # print(f\"Precision: {precision:.2f}\")\n",
    "                # print(f\"Recall: {recall:.2f}\")\n",
    "                # print(f\"F1 Score: {f1:.2f}\")\n",
    "                # print(f\"Roc AUC: {roc_auc:.2f}\")\n",
    "\n",
    "                table_data.append([THRESHOLD, \n",
    "                                timesteps, \n",
    "                                distribution.name, \n",
    "                                regression, \n",
    "                                    str(int(np.round(accuracy, 2)*100))+str('%'), \n",
    "                                    str(int(np.round(precision, 2)*100))+str('%'), \n",
    "                                    str(int(np.round(recall, 2)*100))+str('%'), \n",
    "                                    str(int(np.round(f1, 2)*100))+str('%'), \n",
    "                                    str(int(np.round(roc_auc, 2)*100))+str('%'), \n",
    "                                    test_size])\n",
    "                \n",
    "            \n",
    "            if regression == 'Survival_Analysis':\n",
    "                    # Implement Survival Analysis logic here\n",
    "                    trained_regressors[str(regression)]=[regression]\n",
    "\n",
    "                    scores_cph_tree = {}\n",
    "                    scores_RandomSurvivalForest = {} \n",
    "\n",
    "                    est_cph_tree = RandomSurvivalForest(random_state=random_state)\n",
    "                    for i in range(1, 61):\n",
    "                        n_estimators = i * 5\n",
    "                        est_cph_tree.set_params(n_estimators=n_estimators)\n",
    "                        est_cph_tree.fit(X_train_survival, y_train_survival)\n",
    "                        scores_cph_tree[n_estimators] = est_cph_tree.score(X_test_survival, y_test_survival)\n",
    "                        scores_RandomSurvivalForest['scores_cph_tree'+str(n_estimators)] =  pickle.dumps(est_cph_tree)\n",
    "                    \n",
    "\n",
    "                    metrics = []\n",
    "                    prob_percentages = [0.85, 0.90]\n",
    "\n",
    "                    for i in range(5, 61, 5):\n",
    "                        for prob_percentage in prob_percentages:\n",
    "                            list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "                            y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "                            y_pred_binary = pd.DataFrame(list_of_lists[0:,0], columns=['probabilities'])['probabilities'].apply(lambda x: 1 if x < prob_percentage else 0).values\n",
    "\n",
    "                            # Calculate metrics\n",
    "                            accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "                            precision = precision_score(y_test_, y_pred_binary)\n",
    "                            recall = recall_score(y_test_, y_pred_binary)\n",
    "                            f1 = f1_score(y_test_, y_pred_binary)\n",
    "                            roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "                            \n",
    "\n",
    "                            metrics.append({'model':'scores_cph_tree'+str(i),\n",
    "                                            'accuracy': accuracy,\n",
    "                                            'precision': precision,\n",
    "                                            'recall': recall,\n",
    "                                            'roc_auc':roc_auc,\n",
    "                                            'f1-score':f1,\n",
    "                                            'y_pred_binary':y_pred_binary})\n",
    "\n",
    "                            print(accuracy,'%', precision,'%', recall,'%', f1,'%', roc_auc,'%')\n",
    "\n",
    "                    # Sort the list based on the 'age' field\n",
    "                    sorted_data = sorted(metrics, key=lambda x: x['f1-score'], reverse=True)\n",
    "\n",
    "                    accuracy = sorted_data[0]['accuracy']\n",
    "                    precision = sorted_data[0]['precision']\n",
    "                    recall = sorted_data[0]['recall']    \n",
    "                    f1 = sorted_data[0]['f1-score']\n",
    "                    roc_auc = sorted_data[0]['roc_auc']\n",
    "\n",
    "                    trained_regressors[str(regression)].append(y_pred_binary)\n",
    "                    \n",
    "                    table_data.append([ THRESHOLD, \n",
    "                                    timesteps, \n",
    "                                    None, \n",
    "                                    regression+'_'+str(sorted_data[0]['model']), \n",
    "                                    str(int(np.round(accuracy,2)*100))+str('%'), \n",
    "                                    str(int(np.round(precision,2)*100))+str('%'), \n",
    "                                    str(int(np.round(recall,2)*100))+str('%'), \n",
    "                                    str(int(np.round(f1,2)*100))+str('%'), \n",
    "                                    str(int(np.round(roc_auc,2)*100))+str('%'), \n",
    "                                    test_size])\n",
    "\n",
    "                    \n",
    "                    trained_regressors[str(regression)].append([accuracy, precision, recall, f1, roc_auc])\n",
    "\n",
    "            if regression=='Survival_Analysis_manual_extracted_features':\n",
    "                # Implement Survival Analysis logic here\n",
    "                trained_regressors[str(regression)]=[regression]\n",
    "\n",
    "                scores_cph_tree = {}\n",
    "                scores_RandomSurvivalForest = {} \n",
    "\n",
    "                est_cph_tree = RandomSurvivalForest(random_state=random_state)\n",
    "                for i in range(1, 61):\n",
    "                    n_estimators = i * 5\n",
    "                    est_cph_tree.set_params(n_estimators=n_estimators)\n",
    "                    est_cph_tree.fit(X_train_mn_ext_feat, y_train_mn_ext_feat)\n",
    "                    scores_cph_tree[n_estimators] = est_cph_tree.score(X_test_mn_ext_feat, y_test_mn_ext_feat)\n",
    "                    scores_RandomSurvivalForest['scores_cph_tree'+str(n_estimators)] =  pickle.dumps(est_cph_tree)\n",
    "\n",
    "                metrics = []\n",
    "                prob_percentages = [0.85, 0.90]\n",
    "\n",
    "                for i in range(5, 61, 5):\n",
    "                    for prob_percentage in prob_percentages:\n",
    "                        list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_test_mn_ext_feat, return_array=True)\n",
    "\n",
    "                        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "                        y_pred_binary = pd.DataFrame(list_of_lists[0:,0], columns=['probabilities'])['probabilities'].apply(lambda x: 1 if x < prob_percentage else 0).values\n",
    "\n",
    "                        # Calculate metrics\n",
    "                        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "                        precision = precision_score(y_test_, y_pred_binary)\n",
    "                        recall = recall_score(y_test_, y_pred_binary)\n",
    "                        f1 = f1_score(y_test_, y_pred_binary)\n",
    "                        roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "                        \n",
    "\n",
    "                        metrics.append({'model':'scores_cph_tree'+str(i),\n",
    "                                        'accuracy': accuracy,\n",
    "                                        'precision': precision,\n",
    "                                        'recall': recall,\n",
    "                                        'roc_auc':roc_auc,\n",
    "                                        'f1-score':f1,\n",
    "                                        'y_pred_binary':y_pred_binary})\n",
    "\n",
    "                        print(accuracy,'%', precision,'%', recall,'%', f1,'%', roc_auc,'%')\n",
    "\n",
    "                # Sort the list based on the 'age' field\n",
    "                sorted_data = sorted(metrics, key=lambda x: x['f1-score'], reverse=True)\n",
    "\n",
    "                accuracy = sorted_data[0]['accuracy']\n",
    "                precision = sorted_data[0]['precision']\n",
    "                recall = sorted_data[0]['recall']    \n",
    "                f1 = sorted_data[0]['f1-score']\n",
    "                roc_auc = sorted_data[0]['roc_auc']\n",
    "\n",
    "                trained_regressors[str(regression)].append(y_pred_binary)\n",
    "                \n",
    "                table_data.append([ THRESHOLD, \n",
    "                                    timesteps, \n",
    "                                    None, \n",
    "                                    regression+'_'+str(sorted_data[0]['model']), \n",
    "                                    str(int(np.round(accuracy,2)*100))+str('%'), \n",
    "                                    str(int(np.round(precision,2)*100))+str('%'), \n",
    "                                    str(int(np.round(recall,2)*100))+str('%'), \n",
    "                                    str(int(np.round(f1,2)*100))+str('%'), \n",
    "                                    str(int(np.round(roc_auc,2)*100))+str('%'), \n",
    "                                    test_size])\n",
    "\n",
    "                \n",
    "                trained_regressors[str(regression)].append([accuracy, precision, recall, f1, roc_auc])\n",
    "\n",
    "            \n",
    "            if regression=='Survival_Analysis_tsfel_extracted_features':\n",
    "                # Implement Survival Analysis logic here\n",
    "                trained_regressors[str(regression)]=[regression]\n",
    "\n",
    "                scores_cph_tree = {}\n",
    "                scores_RandomSurvivalForest = {} \n",
    "\n",
    "                est_cph_tree = RandomSurvivalForest(random_state=random_state)\n",
    "                for i in range(1, 61):\n",
    "                    n_estimators = i * 5\n",
    "                    est_cph_tree.set_params(n_estimators=n_estimators)\n",
    "                    est_cph_tree.fit(X_train_tsfel, y_train_tsfel)\n",
    "                    scores_cph_tree[n_estimators] = est_cph_tree.score(X_test_tsfel, y_test_tsfel)\n",
    "                    scores_RandomSurvivalForest['scores_cph_tree'+str(n_estimators)] =  pickle.dumps(est_cph_tree)\n",
    "\n",
    "                metrics = []\n",
    "                prob_percentages = [0.85, 0.90]\n",
    "\n",
    "                for i in range(5, 61, 5):\n",
    "                    for prob_percentage in prob_percentages:\n",
    "                        list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_test_tsfel, return_array=True)\n",
    "\n",
    "                        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "                        y_pred_binary = pd.DataFrame(list_of_lists[0:,0], columns=['probabilities'])['probabilities'].apply(lambda x: 1 if x < prob_percentage else 0).values\n",
    "\n",
    "                        # Calculate metrics\n",
    "                        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "                        precision = precision_score(y_test_, y_pred_binary)\n",
    "                        recall = recall_score(y_test_, y_pred_binary)\n",
    "                        f1 = f1_score(y_test_, y_pred_binary)\n",
    "                        roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "                        \n",
    "\n",
    "                        metrics.append({'model':'scores_cph_tree'+str(i),\n",
    "                                        'accuracy': accuracy,\n",
    "                                        'precision': precision,\n",
    "                                        'recall': recall,\n",
    "                                        'roc_auc':roc_auc,\n",
    "                                        'f1-score':f1,\n",
    "                                        'y_pred_binary':y_pred_binary})\n",
    "\n",
    "                        print(accuracy,'%', precision,'%', recall,'%', f1,'%', roc_auc,'%')\n",
    "\n",
    "                # Sort the list based on the 'age' field\n",
    "                sorted_data = sorted(metrics, key=lambda x: x['f1-score'], reverse=True)\n",
    "\n",
    "                accuracy = sorted_data[0]['accuracy']\n",
    "                precision = sorted_data[0]['precision']\n",
    "                recall = sorted_data[0]['recall']    \n",
    "                f1 = sorted_data[0]['f1-score']\n",
    "                roc_auc = sorted_data[0]['roc_auc']\n",
    "\n",
    "                trained_regressors[str(regression)].append(y_pred_binary)\n",
    "                \n",
    "                table_data.append([ THRESHOLD, \n",
    "                                    timesteps, \n",
    "                                    None, \n",
    "                                    regression+'_'+str(sorted_data[0]['model']), \n",
    "                                    str(int(np.round(accuracy,2)*100))+str('%'), \n",
    "                                    str(int(np.round(precision,2)*100))+str('%'), \n",
    "                                    str(int(np.round(recall,2)*100))+str('%'), \n",
    "                                    str(int(np.round(f1,2)*100))+str('%'), \n",
    "                                    str(int(np.round(roc_auc,2)*100))+str('%'), \n",
    "                                    test_size])\n",
    "\n",
    "                \n",
    "                trained_regressors[str(regression)].append([accuracy, precision, recall, f1, roc_auc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(table_data, headers=[\n",
    "                                    \"Threshold\", \n",
    "                                    \"Number of lags\", \n",
    "                                    \"Distribution\", \n",
    "                                    \"Algorithm\", \n",
    "                                    \"Accuracy\", \n",
    "                                    \"Precision\", \n",
    "                                    \"Recall\", \n",
    "                                    \"F1-score\", \n",
    "                                    'Roc AUC', \n",
    "                                    'Test size'], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "from tabulate import tabulate\n",
    "\n",
    "table_str = tabulate(table_data, headers=[\n",
    "                                    \"Threshold\", \n",
    "                                    \"Number of lags\", \n",
    "                                    \"Distribution\", \n",
    "                                    \"Algorithm\", \n",
    "                                    \"Accuracy\", \n",
    "                                    \"Precision\", \n",
    "                                    \"Recall\", \n",
    "                                    \"F1-score\", \n",
    "                                    'Roc AUC', \n",
    "                                    'Test size'],tablefmt=\"pipe\")\n",
    "\n",
    "ingredient = 'fruits_and_verg'\n",
    "\n",
    "def get_results_df():\n",
    "\n",
    "    # Return the results dataframe\n",
    "    df = pd.read_csv(io.StringIO(table_str), sep=\"|\")\n",
    "    df.to_csv(\n",
    "              '/home/gmarinos/Documents/Code/extended_version_1st_pub/results/'+'_'+ingredient+'_'+'threshold'+'_'+str(np.round(THRESHOLD,0))+'.'+'csv', index=False)\n",
    "    # results.to_csv('/home/gmarinos/Documents/Code/extended_version_1st_pub/results'+'_'+ingredient+'_'+'threshold'+'_'+str(np.round(THRESHOLD,0))+'.'+'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "all_results = pd.read_csv('/home/gmarinos/Documents/Code/extended_version_1st_pub/results/_fruits_and_verg_threshold_11.csv').drop(columns=['Unnamed: 0', 'Unnamed: 11']).iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with percentage values\n",
    "all_results[' Accuracy   '] = all_results[' Accuracy   '].str.replace('%', '')\n",
    "all_results[' Precision   '] = all_results[' Precision   '].str.replace('%', '')\n",
    "all_results[' Recall   '] = all_results[' Recall   '].str.replace('%', '')\n",
    "all_results[' F1-score   '] = all_results[' F1-score   '].str.replace('%', '')\n",
    "all_results[' Roc AUC   '] = all_results[' Roc AUC   '].str.replace('%', '')\n",
    "\n",
    "# Convert the columns to numeric values\n",
    "all_results[' Accuracy   '] = pd.to_numeric(all_results[' Accuracy   '])\n",
    "all_results[' Precision   '] = pd.to_numeric(all_results[' Precision   '])\n",
    "all_results[' Recall   '] = pd.to_numeric(all_results[' Recall   '])\n",
    "all_results[' F1-score   '] = pd.to_numeric(all_results[' F1-score   '])\n",
    "all_results[' Roc AUC   '] = pd.to_numeric(all_results[' Roc AUC   '])\n",
    "\n",
    "all_results['   Threshold '] = pd.to_numeric(all_results['   Threshold '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[['   Threshold ', ' Accuracy   ', ' Precision   ', ' Recall   ', ' F1-score   ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# You can load your data using df = pd.read_csv('your_data.csv') or create it programmatically\n",
    "\n",
    "# Select relevant columns for boxplots\n",
    "metrics = [ ' Accuracy   ', ' Precision   ', ' Recall   ', ' F1-score   ']\n",
    "selected_df = all_results[[' Algorithm                                              '] + metrics]\n",
    "\n",
    "# Melt the DataFrame to long format for seaborn\n",
    "melted_df = pd.melt(selected_df, id_vars=[' Algorithm                                              '], value_vars=metrics, var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(50, 28))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot using seaborn\n",
    "sns.boxplot(x='Metric', y='Value', hue=' Algorithm                                              ', data=melted_df, palette=\"Set3\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Algorithm Performance Comparison')\n",
    "\n",
    "# Adjust the legend position\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### here is also a good visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# You can load your data using df = pd.read_csv('your_data.csv') or create it programmatically\n",
    "\n",
    "# Select relevant columns for boxplots\n",
    "metrics = [ ' Accuracy   ', ' Precision   ', ' Recall   ', ' F1-score   ']\n",
    "selected_df = all_results[[' Algorithm                                              '] + metrics]\n",
    "\n",
    "# Melt the DataFrame to long format for seaborn\n",
    "melted_df = pd.melt(selected_df, id_vars=[' Algorithm                                              '], value_vars=metrics, var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot using seaborn\n",
    "ax = sns.boxplot(x='Metric', y='Value', hue=' Algorithm                                              ', data=melted_df, palette=\"Set3\")\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Algorithm Performance Comparison')\n",
    "\n",
    "# Adjust the legend position\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### that;s a good visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# You can load your data using df = pd.read_csv('your_data.csv') or create it programmatically\n",
    "\n",
    "# Select relevant columns for boxplots\n",
    "metrics = [' Accuracy   ', ' Precision   ', ' Recall   ', ' F1-score   ']\n",
    "selected_df = all_results[[' Algorithm                                              '] + metrics]\n",
    "\n",
    "# Melt the DataFrame to long format for seaborn\n",
    "melted_df = pd.melt(selected_df, id_vars=[' Algorithm                                              '], value_vars=metrics, var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set up a color palette with unique colors\n",
    "unique_colors = sns.color_palette('husl', n_colors=len(selected_df[' Algorithm                                              '].unique()))\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot using seaborn with unique colors\n",
    "ax = sns.boxplot(x='Metric', y='Value', hue=' Algorithm                                              ', data=melted_df, palette=unique_colors)\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Algorithm Performance Comparison')\n",
    "\n",
    "# Adjust the legend position\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### below is a good visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# You can load your data using df = pd.read_csv('your_data.csv') or create it programmatically\n",
    "\n",
    "# Define the list of algorithms for the two categories\n",
    "first_category_algorithms = [\n",
    "    'DecisionTreeRegressor(random_state=42)',\n",
    "       'RandomForestRegressor(random_state=42)',\n",
    "    #    'SGDRegressor(random_state=42)',\n",
    "    #    'KNeighborsRegressor(n_neighbors=3)',\n",
    "    #    'AdaBoostRegressor(random_state=42)', 'BayesianRidge()',\n",
    "    #    'ExtraTreesRegressor(random_state=42)',\n",
    "    #    'BaggingRegressor(random_state=42)',\n",
    "    #    \"LGBMRegressor(objective='regression', random_state=42)\",\n",
    "    #    \"SVR(C=0.025, kernel='linear')\", 'SVR(C=1, gamma=2)',\n",
    "    #    'MLPRegressor(random_state=42)',\n",
    "]\n",
    "\n",
    "second_category_algorithms = [\n",
    "    'Survival_Analysis_scores_cph_tree5',\n",
    "       'Survival_Analysis_scores_cph_tree10',\n",
    "       'Survival_Analysis_scores_cph_tree25'\n",
    "]\n",
    "\n",
    "# Add a new column 'Category' based on algorithm membership\n",
    "all_results[' Algorithm                                              '] = all_results[' Algorithm                                              '].str.strip()\n",
    "all_results['Category'] = all_results[' Algorithm                                              '].apply(lambda x: 'Category 1' if x in first_category_algorithms else 'Category 2')\n",
    "\n",
    "# Select relevant columns for boxplots\n",
    "metrics = [' Accuracy   ', ' Precision   ', ' Recall   ', ' F1-score   ']\n",
    "selected_df = all_results[['Category'] + metrics]\n",
    "\n",
    "# Melt the DataFrame to long format for seaborn\n",
    "melted_df = pd.melt(selected_df, id_vars=['Category'], value_vars=metrics, var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set up a color palette with unique colors for each category\n",
    "palette = {'Category 1': 'lightblue', 'Category 2': 'lightcoral'}\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot using seaborn with unique colors\n",
    "ax = sns.boxplot(x='Metric', y='Value', hue='Category', data=melted_df, palette=palette)\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Algorithm Performance Comparison (Category 1 vs Category 2)')\n",
    "\n",
    "# Adjust the legend position\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization based on algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select relevant columns for boxplots\n",
    "metrics = ['   accuracy ','   precision ', '    recall ', '        f1 ']\n",
    "selected_df = new_results[[' Algorithm                            '] + metrics]\n",
    "\n",
    "# Melt the DataFrame to long format for seaborn\n",
    "melted_df = pd.melt(selected_df, id_vars=[' Algorithm                            '], value_vars=metrics, var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set up a color palette with unique colors\n",
    "unique_colors = sns.color_palette('husl', n_colors=len(selected_df[' Algorithm                            '].unique()))\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(50, 30))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot using seaborn with unique colors\n",
    "ax = sns.boxplot(x='Metric', y='Value', hue=' Algorithm                            ', data=melted_df, palette=unique_colors)\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Algorithm Performance Comparison')\n",
    "\n",
    "# Adjust the legend position\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# You can load your data using df = pd.read_csv('your_data.csv') or create it programmatically\n",
    "\n",
    "\n",
    "# Define the list of algorithms for the two categories\n",
    "first_category_algorithms = [\n",
    "    # 'scores_cph_tree5_risk_prediction',\n",
    "    #    'scores_cph_tree6_risk_prediction',\n",
    "    #    'scores_cph_tree7_risk_prediction',\n",
    "    #    'scores_cph_tree8_risk_prediction',\n",
    "    #    'scores_cph_tree9_risk_prediction',\n",
    "    #    'scores_cph_tree10_risk_prediction',\n",
    "    #    'scores_cph_tree11_risk_prediction',\n",
    "    #    'scores_cph_tree12_risk_prediction',\n",
    "    #    'scores_cph_tree13_risk_prediction',\n",
    "    #    'scores_cph_tree14_risk_prediction',\n",
    "    #    'scores_cph_tree15_risk_prediction',\n",
    "    #    'scores_cph_tree16_risk_prediction',\n",
    "    #    'scores_cph_tree17_risk_prediction',\n",
    "    #    'scores_cph_tree18_risk_prediction',\n",
    "    #    'scores_cph_tree19_risk_prediction',\n",
    "    #    'scores_cph_tree20_risk_prediction',\n",
    "    #    'scores_cph_tree21_risk_prediction',\n",
    "    #    'scores_cph_tree22_risk_prediction',\n",
    "    #    'scores_cph_tree23_risk_prediction',\n",
    "    #    'scores_cph_tree24_risk_prediction',\n",
    "    #    'scores_cph_tree25_risk_prediction',\n",
    "    #    'scores_cph_tree26_risk_prediction',\n",
    "    #    'scores_cph_tree27_risk_prediction',\n",
    "    #    'scores_cph_tree28_risk_prediction',\n",
    "    #    'scores_cph_tree29_risk_prediction',\n",
    "    #    'scores_cph_tree30_risk_prediction',\n",
    "    #    'scores_cph_tree31_risk_prediction',\n",
    "    #    'scores_cph_tree32_risk_prediction',\n",
    "    #    'scores_cph_tree33_risk_prediction',\n",
    "    #    'scores_cph_tree34_risk_prediction',\n",
    "    #    'scores_cph_tree35_risk_prediction',\n",
    "    #    'scores_cph_tree36_risk_prediction',\n",
    "    #    'scores_cph_tree37_risk_prediction',\n",
    "    #    'scores_cph_tree38_risk_prediction',\n",
    "    #    'scores_cph_tree39_risk_prediction',\n",
    "    #    'scores_cph_tree40_risk_prediction',\n",
    "    #    'scores_cph_tree41_risk_prediction',\n",
    "    #    'scores_cph_tree42_risk_prediction',\n",
    "    #    'scores_cph_tree43_risk_prediction',\n",
    "    #    'scores_cph_tree44_risk_prediction',\n",
    "    #    'scores_cph_tree45_risk_prediction',\n",
    "    #    'scores_cph_tree46_risk_prediction',\n",
    "    #    'scores_cph_tree47_risk_prediction',\n",
    "    #    'scores_cph_tree48_risk_prediction',\n",
    "    #    'scores_cph_tree49_risk_prediction',\n",
    "    #    'scores_cph_tree50_risk_prediction',\n",
    "    #    'scores_cph_tree51_risk_prediction',\n",
    "    #    'scores_cph_tree52_risk_prediction',\n",
    "    #    'scores_cph_tree53_risk_prediction',\n",
    "    #    'scores_cph_tree54_risk_prediction',\n",
    "    #    'scores_cph_tree55_risk_prediction',\n",
    "    #    'scores_cph_tree56_risk_prediction',\n",
    "    #    'scores_cph_tree57_risk_prediction',\n",
    "    #    'scores_cph_tree58_risk_prediction',\n",
    "    #    'scores_cph_tree59_risk_prediction',\n",
    "    #    'scores_cph_tree60_risk_prediction',\n",
    "\n",
    "       'CoxPHSurvivalAnalysis_risk_prediction'\n",
    "\n",
    "]\n",
    "\n",
    "second_category_algorithms = [\n",
    "    'scores_cph_tree5_survival_prob_adjusted',\n",
    "       'scores_cph_tree10_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree15_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree20_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree25_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree30_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree35_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree40_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree45_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree50_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree55_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree60_mean_survival_prob_adjusted'\n",
    "\n",
    "]\n",
    "\n",
    "third_category_algorithms = [\n",
    "    'LinearRegressionBinaryCLF'\n",
    "    ]\n",
    "\n",
    "fourth_category_algorithms = [\n",
    "    'LinearRegressionBinaryCLF_Cerceira'\n",
    "    ]\n",
    "\n",
    "fifth_category_algorithms = [\n",
    "    # 'scores_cph_tree5_mean_survival_prob',\n",
    "    #    'scores_cph_tree10_mean_survival_prob',\n",
    "    #    'scores_cph_tree15_mean_survival_prob',\n",
    "    #    'scores_cph_tree20_mean_survival_prob',\n",
    "    #    'scores_cph_tree25_mean_survival_prob',\n",
    "    #    'scores_cph_tree30_mean_survival_prob',\n",
    "    #    'scores_cph_tree35_mean_survival_prob',\n",
    "    #    'scores_cph_tree40_mean_survival_prob',\n",
    "    #    'scores_cph_tree45_mean_survival_prob',\n",
    "    #    'scores_cph_tree50_mean_survival_prob',\n",
    "    #    'scores_cph_tree55_mean_survival_prob',\n",
    "    #    'scores_cph_tree60_mean_survival_prob'\n",
    "\n",
    "    ' scores_cph_tree60_survival_prob',\n",
    "       'scores_cph_tree5_mean_survival_prob',\n",
    "       'scores_cph_tree10_mean_survival_prob',\n",
    "       'scores_cph_tree15_mean_survival_prob',\n",
    "       'scores_cph_tree20_mean_survival_prob',\n",
    "       'scores_cph_tree25_mean_survival_prob',\n",
    "       'scores_cph_tree30_mean_survival_prob',\n",
    "       'scores_cph_tree35_mean_survival_prob',\n",
    "       'scores_cph_tree40_mean_survival_prob',\n",
    "       'scores_cph_tree45_mean_survival_prob',\n",
    "       'scores_cph_tree50_mean_survival_prob',\n",
    "       'scores_cph_tree55_mean_survival_prob',\n",
    "       'scores_cph_tree60_mean_survival_prob'\n",
    "\n",
    "       ]\n",
    "\n",
    "# Add a new column 'Category' based on algorithm membership\n",
    "new_results[' Algorithm                                     '] = new_results[' Algorithm                                     '].str.strip()\n",
    "\n",
    "#new_results['Category'] = new_results[' Algorithm                         '].apply(lambda x: 'Category 1' if x in first_category_algorithms else 'Category 2')\n",
    "\n",
    "#new_results['Category'] = new_results[' Algorithm                         '].apply(lambda x: 'Risk Prediction' if x in first_category_algorithms else ('Survival Probability' if x in second_category_algorithms else 'Binary Classifier'))\n",
    "\n",
    "new_results['Category'] = new_results[' Algorithm                                     '].apply(lambda x: \n",
    "    'Risk Prediction' if x in first_category_algorithms else \n",
    "    ('Mean Survival Probability' if x in fifth_category_algorithms else \n",
    "    ('Binary Classifier' if x in third_category_algorithms else \n",
    "    'Cerceira Binary Classifier')))\n",
    "\n",
    "# new_results['Category'] = new_results[' Algorithm                            '].apply(lambda x: \n",
    "#     'Risk Prediction' if x in first_category_algorithms else \n",
    "#     ('Survival Probability' if x in second_category_algorithms else \n",
    "#     ('Binary Classifier' if x in third_category_algorithms else \n",
    "#     ('Cerceira Binary Classifier' if x in fourth_category_algorithms else \n",
    "#     'Mean Survival Probability'))))\n",
    "\n",
    "\n",
    "# Select relevant columns for boxplots\n",
    "metrics = ['   accuracy ','   precision ', '    recall ', '        f1 ']\n",
    "selected_df = new_results[['Category'] + metrics]\n",
    "\n",
    "# Melt the DataFrame to long format for seaborn\n",
    "melted_df = pd.melt(selected_df, id_vars=['Category'], value_vars=metrics, var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set up a color palette with unique colors for each category\n",
    "palette = {'Risk Prediction': 'lightgreen', \n",
    "           'Survival Probability': 'lightgrey',  \n",
    "           'Binary Classifier': 'lightblue', \n",
    "           'Cerceira Binary Classifier': 'lightyellow', \n",
    "           'Mean Survival Probability':'lightcoral'}\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot using seaborn with unique colors\n",
    "ax = sns.boxplot(x='Metric', y='Value', hue='Category', data=melted_df, palette=palette)\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title({algorithm_name})\n",
    "\n",
    "# Adjust the legend position\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "file_path = os.path.join('/home/gmarinos/Documents/Code/extended_version_1st_pub/plots', algorithm_name)\n",
    "plt.savefig(file_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# You can load your data using df = pd.read_csv('your_data.csv') or create it programmatically\n",
    "\n",
    "\n",
    "# Define the list of algorithms for the two categories\n",
    "first_category_algorithms = [\n",
    "    # 'scores_cph_tree5_risk_prediction',\n",
    "    #    'scores_cph_tree6_risk_prediction',\n",
    "    #    'scores_cph_tree7_risk_prediction',\n",
    "    #    'scores_cph_tree8_risk_prediction',\n",
    "    #    'scores_cph_tree9_risk_prediction',\n",
    "    #    'scores_cph_tree10_risk_prediction',\n",
    "    #    'scores_cph_tree11_risk_prediction',\n",
    "    #    'scores_cph_tree12_risk_prediction',\n",
    "    #    'scores_cph_tree13_risk_prediction',\n",
    "    #    'scores_cph_tree14_risk_prediction',\n",
    "    #    'scores_cph_tree15_risk_prediction',\n",
    "    #    'scores_cph_tree16_risk_prediction',\n",
    "    #    'scores_cph_tree17_risk_prediction',\n",
    "    #    'scores_cph_tree18_risk_prediction',\n",
    "    #    'scores_cph_tree19_risk_prediction',\n",
    "    #    'scores_cph_tree20_risk_prediction',\n",
    "    #    'scores_cph_tree21_risk_prediction',\n",
    "    #    'scores_cph_tree22_risk_prediction',\n",
    "    #    'scores_cph_tree23_risk_prediction',\n",
    "    #    'scores_cph_tree24_risk_prediction',\n",
    "    #    'scores_cph_tree25_risk_prediction',\n",
    "    #    'scores_cph_tree26_risk_prediction',\n",
    "    #    'scores_cph_tree27_risk_prediction',\n",
    "    #    'scores_cph_tree28_risk_prediction',\n",
    "    #    'scores_cph_tree29_risk_prediction',\n",
    "    #    'scores_cph_tree30_risk_prediction',\n",
    "    #    'scores_cph_tree31_risk_prediction',\n",
    "    #    'scores_cph_tree32_risk_prediction',\n",
    "    #    'scores_cph_tree33_risk_prediction',\n",
    "    #    'scores_cph_tree34_risk_prediction',\n",
    "    #    'scores_cph_tree35_risk_prediction',\n",
    "    #    'scores_cph_tree36_risk_prediction',\n",
    "    #    'scores_cph_tree37_risk_prediction',\n",
    "    #    'scores_cph_tree38_risk_prediction',\n",
    "    #    'scores_cph_tree39_risk_prediction',\n",
    "    #    'scores_cph_tree40_risk_prediction',\n",
    "    #    'scores_cph_tree41_risk_prediction',\n",
    "    #    'scores_cph_tree42_risk_prediction',\n",
    "    #    'scores_cph_tree43_risk_prediction',\n",
    "    #    'scores_cph_tree44_risk_prediction',\n",
    "    #    'scores_cph_tree45_risk_prediction',\n",
    "    #    'scores_cph_tree46_risk_prediction',\n",
    "    #    'scores_cph_tree47_risk_prediction',\n",
    "    #    'scores_cph_tree48_risk_prediction',\n",
    "    #    'scores_cph_tree49_risk_prediction',\n",
    "    #    'scores_cph_tree50_risk_prediction',\n",
    "    #    'scores_cph_tree51_risk_prediction',\n",
    "    #    'scores_cph_tree52_risk_prediction',\n",
    "    #    'scores_cph_tree53_risk_prediction',\n",
    "    #    'scores_cph_tree54_risk_prediction',\n",
    "    #    'scores_cph_tree55_risk_prediction',\n",
    "    #    'scores_cph_tree56_risk_prediction',\n",
    "    #    'scores_cph_tree57_risk_prediction',\n",
    "    #    'scores_cph_tree58_risk_prediction',\n",
    "    #    'scores_cph_tree59_risk_prediction',\n",
    "    #    'scores_cph_tree60_risk_prediction',\n",
    "\n",
    "    'scores_cph_tree5_risk_prediction',\n",
    "       'scores_cph_tree10_risk_prediction',\n",
    "       'scores_cph_tree15_risk_prediction',\n",
    "       'scores_cph_tree20_risk_prediction',\n",
    "       'scores_cph_tree25_risk_prediction',\n",
    "       'scores_cph_tree30_risk_prediction',\n",
    "       'scores_cph_tree35_risk_prediction',\n",
    "       'scores_cph_tree40_risk_prediction',\n",
    "       'scores_cph_tree45_risk_prediction',\n",
    "       'scores_cph_tree50_risk_prediction',\n",
    "       'scores_cph_tree55_risk_prediction',\n",
    "       'scores_cph_tree60_risk_prediction'\n",
    "]\n",
    "\n",
    "second_category_algorithms = [\n",
    "    'scores_cph_tree5_survival_prob',\n",
    "       'scores_cph_tree10_survival_prob',\n",
    "       'scores_cph_tree15_survival_prob',\n",
    "       'scores_cph_tree20_survival_prob',\n",
    "       'scores_cph_tree25_survival_prob',\n",
    "       'scores_cph_tree30_survival_prob',\n",
    "       'scores_cph_tree35_survival_prob',\n",
    "       'scores_cph_tree40_survival_prob',\n",
    "       'scores_cph_tree45_survival_prob',\n",
    "       'scores_cph_tree50_survival_prob',\n",
    "       'scores_cph_tree55_survival_prob',\n",
    "       'scores_cph_tree60_survival_prob'\n",
    "]\n",
    "\n",
    "third_category_algorithms = [\n",
    "    'Gradient_Boosting_CLF'\n",
    "    ]\n",
    "\n",
    "fourth_category_algorithms = [\n",
    "    'Gradient_Boosting_Cerceira'\n",
    "    ]\n",
    "\n",
    "fifth_category_algorithms = [\n",
    "    'scores_cph_tree5_mean_survival_prob',\n",
    "       'scores_cph_tree10_mean_survival_prob',\n",
    "       'scores_cph_tree15_mean_survival_prob',\n",
    "       'scores_cph_tree20_mean_survival_prob',\n",
    "       'scores_cph_tree25_mean_survival_prob',\n",
    "       'scores_cph_tree30_mean_survival_prob',\n",
    "       'scores_cph_tree35_mean_survival_prob',\n",
    "       'scores_cph_tree40_mean_survival_prob',\n",
    "       'scores_cph_tree45_mean_survival_prob',\n",
    "       'scores_cph_tree50_mean_survival_prob',\n",
    "       'scores_cph_tree55_mean_survival_prob',\n",
    "       'scores_cph_tree60_mean_survival_prob'\n",
    "       ]\n",
    "\n",
    "\n",
    "sixth_category_algorithms = [\n",
    "    'scores_cph_tree5_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree10_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree15_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree20_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree25_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree30_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree35_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree40_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree45_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree50_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree55_mean_survival_prob_adjusted',\n",
    "       'scores_cph_tree60_mean_survival_prob_adjusted'\n",
    "       ]\n",
    "\n",
    "\n",
    "\n",
    "# Add a new column 'Category' based on algorithm membership\n",
    "new_results[' Algorithm                                     '] = new_results[' Algorithm                                     '].str.strip()\n",
    "\n",
    "#new_results['Category'] = new_results[' Algorithm                         '].apply(lambda x: 'Category 1' if x in first_category_algorithms else 'Category 2')\n",
    "\n",
    "#new_results['Category'] = new_results[' Algorithm                         '].apply(lambda x: 'Risk Prediction' if x in first_category_algorithms else ('Survival Probability' if x in second_category_algorithms else 'Binary Classifier'))\n",
    "\n",
    "# new_results['Category'] = new_results[' Algorithm                                     '].apply(lambda x: \n",
    "#     'Risk Prediction' if x in first_category_algorithms else \n",
    "#     ('Mean Survival Probability' if x in fifth_category_algorithms else \n",
    "#     ('Binary Classifier' if x in third_category_algorithms else \n",
    "#     'Cerceira Binary Classifier')))\n",
    "\n",
    "new_results['Category'] = new_results[' Algorithm                                     '].apply(lambda x: \n",
    "    'Risk Prediction' if x in first_category_algorithms else \n",
    "    ('Old Survival Probability' if x in second_category_algorithms else \n",
    "    ('Binary Classifier' if x in third_category_algorithms else \n",
    "    ('Cerceira Binary Classifier' if x in fourth_category_algorithms else \n",
    "    ('Mean Survival Probability' if x in fifth_category_algorithms else \n",
    "    'Mean Survival Probability Adjusted')))))\n",
    "\n",
    "\n",
    "\n",
    "# Select relevant columns for boxplots\n",
    "metrics = ['   accuracy ','   precision ', '    recall ', '        f1 ']\n",
    "selected_df = new_results[['Category'] + metrics]\n",
    "\n",
    "# Melt the DataFrame to long format for seaborn\n",
    "melted_df = pd.melt(selected_df, id_vars=['Category'], value_vars=metrics, var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set up a color palette with unique colors for each category\n",
    "palette = {'Risk Prediction': 'lightgreen', \n",
    "           'Old Survival Probability': 'lightgrey',  \n",
    "           'Binary Classifier': 'lightblue', \n",
    "           'Cerceira Binary Classifier': 'lightyellow', \n",
    "           'Mean Survival Probability':'lightcoral', \n",
    "           'Mean Survival Probability Adjusted':'black'}\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot using seaborn with unique colors\n",
    "ax = sns.boxplot(x='Metric', y='Value', hue='Category', data=melted_df, palette=palette)\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title(algorithm_name)\n",
    "\n",
    "# Adjust the legend position\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "file_path = os.path.join('/home/gmarinos/Documents/Code/extended_version_1st_pub/plots', algorithm_name)\n",
    "plt.savefig(file_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### only two categories plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# You can load your data using df = pd.read_csv('your_data.csv') or create it programmatically\n",
    "\n",
    "# Define the list of algorithms for the two categories\n",
    "first_category_algorithms = [\n",
    "    'DecisionTreeRegressor(random_state=42)',\n",
    "    'RandomForestRegressor(random_state=42)',\n",
    "    #    'SGDRegressor(random_state=42)',\n",
    "    #    'KNeighborsRegressor(n_neighbors=3)',\n",
    "    #    'AdaBoostRegressor(random_state=42)', 'BayesianRidge()',\n",
    "    #    'ExtraTreesRegressor(random_state=42)',\n",
    "    #    'BaggingRegressor(random_state=42)',\n",
    "    #    \"LGBMRegressor(objective='regression', random_state=42)\",\n",
    "    #    \"SVR(C=0.025, kernel='linear')\", 'SVR(C=1, gamma=2)',\n",
    "    #    'MLPRegressor(random_state=42)',\n",
    "]\n",
    "\n",
    "second_category_algorithms = [\n",
    "    'Survival_Analysis_scores_cph_tree5',\n",
    "       'Survival_Analysis_scores_cph_tree10',\n",
    "       'Survival_Analysis_scores_cph_tree25'\n",
    "]\n",
    "\n",
    "# Add a new column 'Category' based on algorithm membership\n",
    "all_results[' Algorithm                                              '] = all_results[' Algorithm                                              '].str.strip()\n",
    "all_results['Category'] = all_results[' Algorithm                                              '].apply(lambda x: 'Category 1' if x in first_category_algorithms else 'Category 2')\n",
    "\n",
    "# Select relevant columns for boxplots\n",
    "metrics = [' Accuracy   ', ' Precision   ', ' Recall   ', ' F1-score   ']\n",
    "selected_df = all_results[['Category'] + metrics]\n",
    "\n",
    "# Melt the DataFrame to long format for seaborn\n",
    "melted_df = pd.melt(selected_df, id_vars=['Category'], value_vars=metrics, var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set up a color palette with unique colors for each category\n",
    "palette = {'Category 1': 'lightblue', 'Category 2': 'lightcoral'}\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot using seaborn with unique colors\n",
    "ax = sns.boxplot(x='Metric', y='Value', hue='Category', data=melted_df, palette=palette)\n",
    "\n",
    "# Rotate x-axis labels vertically\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Algorithm Performance Comparison (Category 1 vs Category 2)')\n",
    "\n",
    "# Adjust the legend position\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case of a simple/brief experiment use that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest, ExtraSurvivalTrees\n",
    "from sksurv.tree import SurvivalTree\n",
    "from sksurv.svm import FastKernelSurvivalSVM, FastSurvivalSVM, NaiveSurvivalSVM\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "import pickle \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from scipy.stats import norm, laplace, logistic, gumbel_r, lognorm, cauchy, genextreme \n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "scores_RandomSurvivalForest = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "distributions = [genextreme, norm, laplace, logistic, gumbel_r, lognorm, cauchy]\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "# Loop through models and distributions and evaluate each one\n",
    "for name, values in splitted_data.items():\n",
    "\n",
    "    if name == 'fruits and vegetables':\n",
    "    \n",
    "        X_train_survival = values['X_train_survival']\n",
    "        X_test_survival = values['X_test_survival']\n",
    "        y_train_survival = values['y_train_survival']\n",
    "        y_test_survival = values['y_test_survival']\n",
    "\n",
    "        X_train = values['X_train']\n",
    "        y_train = values['y_train']\n",
    "        X_test = values['X_test']\n",
    "        y_test = values['y_test']\n",
    "\n",
    "        X_train_mn_ext_feat = values['X_train_mn_ext_feat']\n",
    "        X_test_mn_ext_feat = values['X_test_mn_ext_feat']\n",
    "        y_train_mn_ext_feat = values['y_train_mn_ext_feat'] \n",
    "        y_test_mn_ext_feat = values['y_test_mn_ext_feat']\n",
    "        \n",
    "        X_train_tsfel = values['X_train_tsfel']\n",
    "        y_train_tsfel = values['y_train_tsfel']\n",
    "        X_test_tsfel = values['X_test_tsfel']\n",
    "        y_test_tsfel = values['y_test_tsfel']\n",
    "\n",
    "        THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "\n",
    "\n",
    "        ## Risk Prediction approach \n",
    "        est_cph_tree = RandomSurvivalForest()\n",
    "        \n",
    "        est_cph_tree.fit(X_train_survival, y_train_survival)\n",
    "        # print('c-index:', est_cph_tree.score(X_test_survival, y_test_survival))\n",
    "\n",
    "\n",
    "        metrics = []\n",
    "\n",
    "        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "        for i in range(5, 61, 5):\n",
    "            list_of_lists = est_cph_tree.predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "            y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "            #Approach 3\n",
    "            predicted_mean_survival_prob = []\n",
    "\n",
    "            for prob in list_of_lists:\n",
    "                predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "            \n",
    "            y_pred_binary = (np.array(predicted_mean_survival_prob) > 0.4).astype(int)\n",
    "\n",
    "\n",
    "            accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "            precision = precision_score(y_test_, y_pred_binary)\n",
    "            recall = recall_score(y_test_, y_pred_binary)\n",
    "            f1 = f1_score(y_test_, y_pred_binary)\n",
    "            roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "            print('Model:', 'scores_cph_tree'+str(i)+'_survival_prob',\n",
    "                'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest, ExtraSurvivalTrees\n",
    "from sksurv.tree import SurvivalTree\n",
    "from sksurv.svm import FastKernelSurvivalSVM\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "import pickle \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from scipy.stats import norm, laplace, logistic, gumbel_r, lognorm, cauchy, genextreme \n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "scores_RandomSurvivalForest = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "distributions = [genextreme, norm, laplace, logistic, gumbel_r, lognorm, cauchy]\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "# Loop through models and distributions and evaluate each one\n",
    "for name, values in splitted_data.items():\n",
    "    \n",
    "    X_train_survival = values['X_train_survival']\n",
    "    X_test_survival = values['X_test_survival']\n",
    "    y_train_survival = values['y_train_survival']\n",
    "    y_test_survival = values['y_test_survival']\n",
    "\n",
    "    X_train = values['X_train']\n",
    "    y_train = values['y_train']\n",
    "    X_test = values['X_test']\n",
    "    y_test = values['y_test']\n",
    "\n",
    "    X_train_mn_ext_feat = values['X_train_mn_ext_feat']\n",
    "    X_test_mn_ext_feat = values['X_test_mn_ext_feat']\n",
    "    y_train_mn_ext_feat = values['y_train_mn_ext_feat'] \n",
    "    y_test_mn_ext_feat = values['y_test_mn_ext_feat']\n",
    "    \n",
    "    X_train_tsfel = values['X_train_tsfel']\n",
    "    y_train_tsfel = values['y_train_tsfel']\n",
    "    X_test_tsfel = values['X_test_tsfel']\n",
    "    y_test_tsfel = values['y_test_tsfel']\n",
    "\n",
    "    THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "\n",
    "\n",
    "    ## Train the Survival analysis algorithm \n",
    "    est_cph_tree = ExtraSurvivalTrees(random_state=random_state)\n",
    "    for i in range(5, 61, 5):\n",
    "        n_estimators = i \n",
    "        est_cph_tree.set_params(n_estimators=n_estimators)\n",
    "        est_cph_tree.fit(X_train_survival, y_train_survival)\n",
    "        # print('c-index:', est_cph_tree.score(X_test_survival, y_test_survival))\n",
    "        scores_RandomSurvivalForest['scores_cph_tree'+str(n_estimators)] =  pickle.dumps(est_cph_tree)\n",
    "\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "    \n",
    "    ###########################################################################################################################################################\n",
    "    ## Risk Prediction approach \n",
    "    \n",
    "    for i in range(5, 61, 5):\n",
    "        predicted_risks = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict(X_test_survival)\n",
    "\n",
    "        #find the guide from the real data \n",
    "        for key, value in enumerate(y_test_):\n",
    "            if value ==1: \n",
    "                guide_n = key\n",
    "                break\n",
    "\n",
    "        guide = predicted_risks[guide_n]\n",
    "\n",
    "        #transform the risks to binary outcome\n",
    "        y_pred_binary = []\n",
    "\n",
    "        for key, value in enumerate(predicted_risks):\n",
    "            if value > guide:\n",
    "                y_pred_binary.append(1)\n",
    "            elif value < guide:\n",
    "                y_pred_binary.append(0)\n",
    "            else: \n",
    "                y_pred_binary.append(1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        #roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_risk-prediction', \n",
    "            'c-index:', np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', None\n",
    "                )\n",
    "        \n",
    "        results.append([name, \n",
    "                        'scores_cph_tree'+str(i)+'_risk-prediction', \n",
    "                        np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        None, \n",
    "                        None])\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Survival Probability approach\n",
    "    ## Approach 1\n",
    "\n",
    "    prob_percentages = [0.85, 0.90]\n",
    "\n",
    "\n",
    "    for i in range(5, 61, 5):\n",
    "        for prob_percentage in prob_percentages:\n",
    "            list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "            y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "            y_pred_binary = pd.DataFrame(list_of_lists[0:,0], columns=['probabilities'])['probabilities'].apply(lambda x: 1 if x < prob_percentage else 0).values\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "            precision = precision_score(y_test_, y_pred_binary)\n",
    "            recall = recall_score(y_test_, y_pred_binary)\n",
    "            f1 = f1_score(y_test_, y_pred_binary)\n",
    "            roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "            print('Model:', 'scores_cph_tree'+str(i)+'_survival-prob',\n",
    "                'c-index:', np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', roc_auc)\n",
    "            \n",
    "            results.append([name, \n",
    "                        'scores_cph_tree'+str(i)+'_survival-prob', \n",
    "                        np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        roc_auc, \n",
    "                        None])\n",
    "            \n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Approach 2\n",
    "\n",
    "    for i in range(5, 61, 5):\n",
    "        list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "        predicted_mean_survival_prob = []\n",
    "\n",
    "        for prob in list_of_lists:\n",
    "            predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "\n",
    "        for key, value in enumerate(y_test_):\n",
    "            if value ==1: \n",
    "                guide_k = key\n",
    "                break\n",
    "\n",
    "        guide_ = predicted_mean_survival_prob[guide_k]\n",
    "\n",
    "        y_pred_binary_ = []\n",
    "\n",
    "        for key, value in enumerate(predicted_mean_survival_prob):\n",
    "            if value > guide_:\n",
    "                y_pred_binary_.append(1)\n",
    "            elif value < guide_:\n",
    "                y_pred_binary_.append(0)\n",
    "            else: \n",
    "                y_pred_binary_.append(1)\n",
    "        \n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary_)\n",
    "        precision = precision_score(y_test_, y_pred_binary_)\n",
    "        recall = recall_score(y_test_, y_pred_binary_)\n",
    "        f1 = f1_score(y_test_, y_pred_binary_)\n",
    "        #roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted',\n",
    "            'c-index:', np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "            'precision:', precision, \n",
    "            'recall:', recall, \n",
    "            'f1-score:', f1, \n",
    "            'roc_auc:', None)\n",
    "        \n",
    "        results.append([name, \n",
    "                    'scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted', \n",
    "                    np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    None, \n",
    "                    None])\n",
    "\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Approach 3\n",
    "\n",
    "    for i in range(5, 61, 5):\n",
    "        list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "        predicted_mean_survival_prob = []\n",
    "\n",
    "        for prob in list_of_lists:\n",
    "            predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "        y_pred_binary = []\n",
    "        y_pred_binary = (np.array(predicted_mean_survival_prob) > 0.5).astype(int)\n",
    "\n",
    "\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        roc_auc = roc_auc_score(y_test_, predicted_mean_survival_prob)\n",
    "\n",
    "        predictions['scores_cph_tree'+str(i)+'_mean-survival-prob'] = y_pred_binary\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_mean-survival-prob',\n",
    "            'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "            'precision:', precision, \n",
    "            'recall:', recall, \n",
    "            'f1-score:', f1, \n",
    "            'roc_auc:', roc_auc)\n",
    "        \n",
    "        \n",
    "        results.append([name, \n",
    "                    'scores_cph_tree'+str(i)+'_mean-survival-prob', \n",
    "                    np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    roc_auc, \n",
    "                    None])\n",
    "             \n",
    "            \n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Vitor Cerceira's approach \n",
    "\n",
    "    regression = ExtraTreesRegressor(random_state=random_state)\n",
    "    for distribution in distributions:\n",
    "        \n",
    "        # Implement the model fitting and evaluation logic here\n",
    "        # print('\\n')\n",
    "        # print('Distribution:', distribution.name)\n",
    "        # print('Model:', str(regression))\n",
    "\n",
    "        regression.fit(X_train, y_train)\n",
    "\n",
    "        # trained_regressors[str(regression)+'_'+str(distribution.name)]=[regression]\n",
    "\n",
    "        # getting point forecasts\n",
    "        point_forecasts = regression.predict(X_test)\n",
    "        std = y_train.std()\n",
    "        if distribution == lognorm:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, s=std) for x_ in point_forecasts])\n",
    "        elif distribution == genextreme:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, c = -0.1, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "        elif distribution != lognorm:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "        \n",
    "        #y_test_ = new_df['marked'].loc[y_test.index]\n",
    "        y_test_ = (y_test > THRESHOLD).astype(int)\n",
    "        \n",
    "        y_pred_binary = np.where(exceedance_prob >= 0.5, 1, 0)\n",
    "        # results['actual_predictions_from_model'+'_'+str(regression)+'_'+str(distribution.name)]  = point_forecasts\n",
    "        # results['y_pred_binary'+'_'+str(regression)+'_'+str(distribution.name)]  = y_pred_binary\n",
    "        # trained_regressors[str(regression)+'_'+str(distribution.name)].append(y_pred_binary)\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(y_test_, y_pred_binary)\n",
    "\n",
    "        # Compute the precision, recall, and F1 score\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        roc_auc = roc_auc_score(y_test_, exceedance_prob)\n",
    "\n",
    "        print('Model:', 'Gradient Boosting Cerceira',\n",
    "          'c-index:', None,\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', roc_auc)\n",
    "    \n",
    "        results.append([name, \n",
    "                        'Gradient_Boosting_Cerceira', \n",
    "                        None,\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        None, \n",
    "                        distribution])\n",
    "\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Standard Binary Classification (Baseline)\n",
    "    rf_classifier = ExtraTreeClassifier(random_state=random_state)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    #THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "    y_train_binary = (y_train > THRESHOLD).astype(int)\n",
    "    rf_classifier.fit(X_train, y_train_binary)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_, y_pred)\n",
    "    precision = precision_score(y_test_, y_pred)\n",
    "    recall = recall_score(y_test_, y_pred)\n",
    "    f1 = f1_score(y_test_, y_pred)\n",
    "\n",
    "    print('Model:', 'Gradient_Boosting Binary Clf',\n",
    "          'c-index:', None,\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', None)\n",
    "    \n",
    "    results.append([name, \n",
    "                    'Gradient_Boosting_CLF', \n",
    "                    None,\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    None, \n",
    "                    None])\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest, ExtraSurvivalTrees\n",
    "from sksurv.tree import SurvivalTree\n",
    "from sksurv.svm import FastKernelSurvivalSVM, FastSurvivalSVM, NaiveSurvivalSVM\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "import pickle \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from scipy.stats import norm, laplace, logistic, gumbel_r, lognorm, cauchy, genextreme \n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "scores_RandomSurvivalForest = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "distributions = [genextreme, norm, laplace, logistic, gumbel_r, lognorm, cauchy]\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "# Loop through models and distributions and evaluate each one\n",
    "for name, values in splitted_data.items():\n",
    "    \n",
    "    X_train_survival = values['X_train_survival']\n",
    "    X_test_survival = values['X_test_survival']\n",
    "    y_train_survival = values['y_train_survival']\n",
    "    y_test_survival = values['y_test_survival']\n",
    "\n",
    "    X_train = values['X_train']\n",
    "    y_train = values['y_train']\n",
    "    X_test = values['X_test']\n",
    "    y_test = values['y_test']\n",
    "\n",
    "    X_train_mn_ext_feat = values['X_train_mn_ext_feat']\n",
    "    X_test_mn_ext_feat = values['X_test_mn_ext_feat']\n",
    "    y_train_mn_ext_feat = values['y_train_mn_ext_feat'] \n",
    "    y_test_mn_ext_feat = values['y_test_mn_ext_feat']\n",
    "    \n",
    "    X_train_tsfel = values['X_train_tsfel']\n",
    "    y_train_tsfel = values['y_train_tsfel']\n",
    "    X_test_tsfel = values['X_test_tsfel']\n",
    "    y_test_tsfel = values['y_test_tsfel']\n",
    "\n",
    "    THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "\n",
    "\n",
    "    ## Risk Prediction approach \n",
    "    est_cph_tree = CoxPHSurvivalAnalysis()\n",
    "    \n",
    "    est_cph_tree.fit(X_train_survival, y_train_survival)\n",
    "    # print('c-index:', est_cph_tree.score(X_test_survival, y_test_survival))\n",
    "\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "    \n",
    "    ###########################################################################################################################################################\n",
    "    # Risk Prediction \n",
    "\n",
    "    for i in range(5, 61, 5):\n",
    "        predicted_risks = est_cph_tree.predict(X_test_survival)\n",
    "\n",
    "        for key, value in enumerate(y_test_):\n",
    "            if value ==1: \n",
    "                guide_n = key\n",
    "                break\n",
    "\n",
    "        guide = predicted_risks[guide_n]\n",
    "\n",
    "        y_pred_binary = []\n",
    "\n",
    "        for key, value in enumerate(predicted_risks):\n",
    "            if value > guide:\n",
    "                y_pred_binary.append(1)\n",
    "            elif value < guide:\n",
    "                y_pred_binary.append(0)\n",
    "            else: \n",
    "                y_pred_binary.append(1)\n",
    "\n",
    "        predictions['CoxPHSurvivalAnalysis'+'_risk-prediction'] = y_pred_binary\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        #roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "\n",
    "\n",
    "        print('Model:', 'CoxPHSurvivalAnalysis'+'_risk-prediction', \n",
    "            'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', None\n",
    "                )\n",
    "        \n",
    "        results.append([name, \n",
    "                        'CoxPHSurvivalAnalysis'+'_risk-prediction', \n",
    "                        np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        None, \n",
    "                        None])\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Survival Probability approach\n",
    "    ## Approach 1 \n",
    "\n",
    "    prob_percentages = [0.85, 0.90]\n",
    "\n",
    "\n",
    "    for prob_percentage in prob_percentages:\n",
    "        list_of_lists = est_cph_tree.predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "        y_pred_binary = pd.DataFrame(list_of_lists[0:,0], columns=['probabilities'])['probabilities'].apply(lambda x: 1 if x < prob_percentage else 0).values\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_survival-prob',\n",
    "            'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "            'precision:', precision, \n",
    "            'recall:', recall, \n",
    "            'f1-score:', f1, \n",
    "            'roc_auc:', roc_auc)\n",
    "        \n",
    "        results.append([name, \n",
    "                    'scores_cph_tree'+str(i)+'_survival-prob', \n",
    "                    np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    roc_auc, \n",
    "                    None])\n",
    "        \n",
    "        predictions['scores_cph_tree'+str(i)+'_survival-prob'] = y_pred_binary\n",
    "            \n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Approach 2\n",
    "\n",
    "    for i in range(5, 61, 5):\n",
    "        list_of_lists = est_cph_tree.predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "        predicted_mean_survival_prob = []\n",
    "\n",
    "        for prob in list_of_lists:\n",
    "            predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "\n",
    "        for key, value in enumerate(y_test_):\n",
    "            if value ==1: \n",
    "                guide_k = key\n",
    "                break\n",
    "\n",
    "        guide_ = predicted_mean_survival_prob[guide_k]\n",
    "\n",
    "        y_pred_binary_ = []\n",
    "\n",
    "        for key, value in enumerate(predicted_mean_survival_prob):\n",
    "            if value > guide_:\n",
    "                y_pred_binary_.append(1)\n",
    "            elif value < guide_:\n",
    "                y_pred_binary_.append(0)\n",
    "            else: \n",
    "                y_pred_binary_.append(1)\n",
    "        \n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary_)\n",
    "        precision = precision_score(y_test_, y_pred_binary_)\n",
    "        recall = recall_score(y_test_, y_pred_binary_)\n",
    "        f1 = f1_score(y_test_, y_pred_binary_)\n",
    "        #roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "\n",
    "        predictions['scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted'] = y_pred_binary_\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted',\n",
    "            'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "            'precision:', precision, \n",
    "            'recall:', recall, \n",
    "            'f1-score:', f1, \n",
    "            'roc_auc:', None)\n",
    "        \n",
    "        results.append([name, \n",
    "                    'scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted', \n",
    "                    np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    None, \n",
    "                    None])\n",
    "            \n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Approach 3\n",
    "    \n",
    "    for i in range(5, 61, 5):\n",
    "        list_of_lists = est_cph_tree.predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "        predicted_mean_survival_prob = []\n",
    "\n",
    "        for prob in list_of_lists:\n",
    "            predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "        y_pred_binary = []\n",
    "        y_pred_binary = (np.array(predicted_mean_survival_prob) > 0.5).astype(int)\n",
    "\n",
    "\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        roc_auc = roc_auc_score(y_test_, predicted_mean_survival_prob)\n",
    "\n",
    "        predictions['scores_cph_tree'+str(i)+'_mean-survival-prob'] = y_pred_binary\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_mean-survival-prob',\n",
    "            'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "            'precision:', precision, \n",
    "            'recall:', recall, \n",
    "            'f1-score:', f1, \n",
    "            'roc_auc:', roc_auc)\n",
    "        \n",
    "        \n",
    "        results.append([name, \n",
    "                    'scores_cph_tree'+str(i)+'_mean-survival-prob', \n",
    "                    np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    roc_auc, \n",
    "                    None])\n",
    "            \n",
    "    print('\\n')\n",
    "\n",
    "    ## Vitor Cerceira's approach \n",
    "\n",
    "    regression = LinearRegression()\n",
    "    for distribution in distributions:\n",
    "        \n",
    "        # Implement the model fitting and evaluation logic here\n",
    "        # print('\\n')\n",
    "        # print('Distribution:', distribution.name)\n",
    "        # print('Model:', str(regression))\n",
    "\n",
    "        regression.fit(X_train, y_train)\n",
    "\n",
    "        # trained_regressors[str(regression)+'_'+str(distribution.name)]=[regression]\n",
    "\n",
    "        # getting point forecasts\n",
    "        point_forecasts = regression.predict(X_test)\n",
    "        std = y_train.std()\n",
    "        if distribution == lognorm:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, s=std) for x_ in point_forecasts])\n",
    "        elif distribution == genextreme:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, c = -0.1, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "        elif distribution != lognorm:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "        \n",
    "        #y_test_ = new_df['marked'].loc[y_test.index]\n",
    "        y_test_ = (y_test > THRESHOLD).astype(int)\n",
    "        \n",
    "        y_pred_binary = np.where(exceedance_prob >= 0.5, 1, 0)\n",
    "        # results['actual_predictions_from_model'+'_'+str(regression)+'_'+str(distribution.name)]  = point_forecasts\n",
    "        # results['y_pred_binary'+'_'+str(regression)+'_'+str(distribution.name)]  = y_pred_binary\n",
    "        # trained_regressors[str(regression)+'_'+str(distribution.name)].append(y_pred_binary)\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(y_test_, y_pred_binary)\n",
    "\n",
    "        # Compute the precision, recall, and F1 score\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        roc_auc = roc_auc_score(y_test_, exceedance_prob)\n",
    "\n",
    "        print('Model:', 'Linear Regression Cerceira',\n",
    "          'c-index:', None,\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', roc_auc)\n",
    "    \n",
    "        results.append([name, \n",
    "                        'LinearRegressionBinaryCLF_Cerceira', \n",
    "                        None,\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        None, \n",
    "                        distribution])\n",
    "        \n",
    "        predictions['LinearRegressionBinaryCLF_Cerceira'] = y_pred_binary\n",
    "\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ## Standard Binary Classification (Baseline)\n",
    "    rf_classifier = LogisticRegression(random_state=random_state)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    #THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "    y_train_binary = (y_train > THRESHOLD).astype(int)\n",
    "    rf_classifier.fit(X_train, y_train_binary)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_, y_pred)\n",
    "    precision = precision_score(y_test_, y_pred)\n",
    "    recall = recall_score(y_test_, y_pred)\n",
    "    f1 = f1_score(y_test_, y_pred)\n",
    "\n",
    "    print('Model:', 'Linear Regression Binary Clf',\n",
    "          'c-index:', None,\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', None)\n",
    "    \n",
    "    results.append([name, \n",
    "                    'LinearRegressionBinaryCLF', \n",
    "                    None,\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    None, \n",
    "                    None])\n",
    "    \n",
    "    predictions['LinearRegressionBinaryCLF'] = y_pred\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest, ExtraSurvivalTrees\n",
    "from sksurv.tree import SurvivalTree\n",
    "from sksurv.svm import FastKernelSurvivalSVM\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "import pickle \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from scipy.stats import norm, laplace, logistic, gumbel_r, lognorm, cauchy, genextreme \n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "scores_RandomSurvivalForest = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "distributions = [genextreme, norm, laplace, logistic, gumbel_r, lognorm, cauchy]\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "# Loop through models and distributions and evaluate each one\n",
    "for name, values in splitted_data.items():\n",
    "    \n",
    "    X_train_survival = values['X_train_survival']\n",
    "    X_test_survival = values['X_test_survival']\n",
    "    y_train_survival = values['y_train_survival']\n",
    "    y_test_survival = values['y_test_survival']\n",
    "\n",
    "    X_train = values['X_train']\n",
    "    y_train = values['y_train']\n",
    "    X_test = values['X_test']\n",
    "    y_test = values['y_test']\n",
    "\n",
    "    X_train_mn_ext_feat = values['X_train_mn_ext_feat']\n",
    "    X_test_mn_ext_feat = values['X_test_mn_ext_feat']\n",
    "    y_train_mn_ext_feat = values['y_train_mn_ext_feat'] \n",
    "    y_test_mn_ext_feat = values['y_test_mn_ext_feat']\n",
    "    \n",
    "    X_train_tsfel = values['X_train_tsfel']\n",
    "    y_train_tsfel = values['y_train_tsfel']\n",
    "    X_test_tsfel = values['X_test_tsfel']\n",
    "    y_test_tsfel = values['y_test_tsfel']\n",
    "\n",
    "    THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "    #THRESHOLD = np.quantile(y_train, 0.85)\n",
    "\n",
    "\n",
    "    ## Train the Survival analysis algorithm \n",
    "    est_cph_tree = RandomSurvivalForest(random_state=random_state)\n",
    "    for i in range(5, 61, 5):\n",
    "        n_estimators = i \n",
    "        est_cph_tree.set_params(n_estimators=n_estimators)\n",
    "        est_cph_tree.fit(X_train_survival, y_train_survival)\n",
    "        # print('c-index:', est_cph_tree.score(X_test_survival, y_test_survival))\n",
    "        scores_RandomSurvivalForest['scores_cph_tree'+str(n_estimators)] =  pickle.dumps(est_cph_tree)\n",
    "\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "    y_train_ = (y_train_survival['extreme_event'] == True).astype(int)\n",
    "    \n",
    "    ###########################################################################################################################################################\n",
    "    ## Risk Prediction approach \n",
    "    ## Approach 1\n",
    "    \n",
    "    for i in range(5, 61, 5):\n",
    "        predicted_risks = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict(X_test_survival)\n",
    "        historical_predicted_risks = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict(X_train_survival)\n",
    "\n",
    "        # #find the guide from the real data \n",
    "        # for key, value in enumerate(y_test_):\n",
    "        #     if value == 1: \n",
    "        #         guide_n = key\n",
    "        #         break\n",
    "\n",
    "        #find the guide from the historical data \n",
    "        for key, value in enumerate(y_train_):\n",
    "            if value == 1: \n",
    "                guide_n = key\n",
    "                break\n",
    "        guide = historical_predicted_risks[guide_n]\n",
    "\n",
    "        # Uncomment the following line \n",
    "        #guide = np.quantile(historical_predicted_risks, 0.8)\n",
    "\n",
    "\n",
    "        ## alternative\n",
    "        # # Select the floats corresponding to 1s in the binary array\n",
    "        # selected_floats = historical_predicted_risks[y_train_ == 1]\n",
    "\n",
    "        # # Find the maximum value among the selected floats\n",
    "        # guide = np.max(selected_floats)\n",
    "\n",
    "\n",
    "        #transform the risks to binary outcome\n",
    "        y_pred_binary = []\n",
    "\n",
    "        for key, value in enumerate(predicted_risks):\n",
    "            if value > guide:\n",
    "                y_pred_binary.append(1)\n",
    "            elif value < guide:\n",
    "                y_pred_binary.append(0)\n",
    "            else: \n",
    "                y_pred_binary.append(1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        #roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_risk-prediction', \n",
    "            'c-index:', np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', None\n",
    "                )\n",
    "        \n",
    "        results.append([name, \n",
    "                        'scores_cph_tree'+str(i)+'_risk-prediction', \n",
    "                        np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        None, \n",
    "                        None])\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Risk Prediction approach \n",
    "    ## Approach 2\n",
    "    \n",
    "    for i in range(5, 61, 5):\n",
    "        predicted_risks = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict(X_test_survival)\n",
    "        historical_predicted_risks = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict(X_train_survival)\n",
    "\n",
    "        #find the guide from the real data \n",
    "        # for key, value in enumerate(y_test_):\n",
    "        #     if value == 1: \n",
    "        #         guide_n = key\n",
    "        #         break\n",
    "\n",
    "        #find the guide from the historical data \n",
    "        for key, value in enumerate(y_train_):\n",
    "            if value == 1: \n",
    "                guide_n = key\n",
    "                break\n",
    "\n",
    "        guide = historical_predicted_risks[guide_n]\n",
    "\n",
    "        #transform the risks to binary outcome\n",
    "        y_pred_binary = []\n",
    "\n",
    "        for key, value in enumerate(predicted_risks):\n",
    "            if value > guide:\n",
    "                y_pred_binary.append(1)\n",
    "            elif value < guide:\n",
    "                y_pred_binary.append(0)\n",
    "            else: \n",
    "                y_pred_binary.append(1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        #roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_risk-prediction-adjusted', \n",
    "            'c-index:', np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', None\n",
    "                )\n",
    "        \n",
    "        results.append([name, \n",
    "                        'scores_cph_tree'+str(i)+'_risk-prediction-adjusted', \n",
    "                        np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        None, \n",
    "                        None])\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Survival Probability approach\n",
    "    ## Approach 1\n",
    "\n",
    "    prob_percentages = [0.85, 0.90]\n",
    "\n",
    "\n",
    "    for i in range(5, 61, 5):\n",
    "        for prob_percentage in prob_percentages:\n",
    "            list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "            y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "            y_pred_binary = pd.DataFrame(list_of_lists[0:,0], columns=['probabilities'])['probabilities'].apply(lambda x: 1 if x < prob_percentage else 0).values\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "            precision = precision_score(y_test_, y_pred_binary)\n",
    "            recall = recall_score(y_test_, y_pred_binary)\n",
    "            f1 = f1_score(y_test_, y_pred_binary)\n",
    "            roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "            print('Model:', 'scores_cph_tree'+str(i)+'_survival-prob',\n",
    "                'c-index:', np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', roc_auc)\n",
    "            \n",
    "            results.append([name, \n",
    "                        'scores_cph_tree'+str(i)+'_survival-prob', \n",
    "                        np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        roc_auc, \n",
    "                        None])\n",
    "            \n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Approach 2\n",
    "\n",
    "    for i in range(5, 61, 5):\n",
    "        list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_test_survival, return_array=True)\n",
    "        historical_list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_train_survival, return_array=True)\n",
    "\n",
    "        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "        y_train_ = (y_train_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "        #Use guide from real data \n",
    "        predicted_mean_survival_prob = []\n",
    "\n",
    "        for prob in list_of_lists:\n",
    "            predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "        #Use guide from historical data\n",
    "        historical_predicted_mean_survival_prob = []\n",
    "\n",
    "        for prob in historical_list_of_lists:\n",
    "            historical_predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "\n",
    "        for key, value in enumerate(y_train_):\n",
    "            if value ==1: \n",
    "                guide_k = key\n",
    "                break\n",
    "\n",
    "        guide_ = historical_predicted_mean_survival_prob[guide_k]\n",
    "\n",
    "        y_pred_binary_ = []\n",
    "\n",
    "        for key, value in enumerate(predicted_mean_survival_prob):\n",
    "            if value > guide_:\n",
    "                y_pred_binary_.append(1)\n",
    "            elif value < guide_:\n",
    "                y_pred_binary_.append(0)\n",
    "            else: \n",
    "                y_pred_binary_.append(1)\n",
    "        \n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary_)\n",
    "        precision = precision_score(y_test_, y_pred_binary_)\n",
    "        recall = recall_score(y_test_, y_pred_binary_)\n",
    "        f1 = f1_score(y_test_, y_pred_binary_)\n",
    "        #roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted',\n",
    "            'c-index:', np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "            'precision:', precision, \n",
    "            'recall:', recall, \n",
    "            'f1-score:', f1, \n",
    "            'roc_auc:', None)\n",
    "        \n",
    "        results.append([name, \n",
    "                    'scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted', \n",
    "                    np.round(pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).score(X_test_survival, y_test_survival),2),\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    None, \n",
    "                    None])\n",
    "\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Approach 3\n",
    "\n",
    "    for i in range(5, 61, 5):\n",
    "        list_of_lists = pickle.loads(scores_RandomSurvivalForest['scores_cph_tree'+str(i)]).predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "        y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "        predicted_mean_survival_prob = []\n",
    "\n",
    "        for prob in list_of_lists:\n",
    "            predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "        y_pred_binary = []\n",
    "        y_pred_binary = (np.array(predicted_mean_survival_prob) > 0.5).astype(int)\n",
    "\n",
    "\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        roc_auc = roc_auc_score(y_test_, predicted_mean_survival_prob)\n",
    "\n",
    "        predictions['scores_cph_tree'+str(i)+'_mean-survival-prob'] = y_pred_binary\n",
    "\n",
    "        print('Model:', 'scores_cph_tree'+str(i)+'_mean-survival-prob',\n",
    "            'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "            'precision:', precision, \n",
    "            'recall:', recall, \n",
    "            'f1-score:', f1, \n",
    "            'roc_auc:', roc_auc)\n",
    "        \n",
    "        \n",
    "        results.append([name, \n",
    "                    'scores_cph_tree'+str(i)+'_mean-survival-prob', \n",
    "                    np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    roc_auc, \n",
    "                    None])\n",
    "             \n",
    "            \n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Vitor Cerceira's approach \n",
    "\n",
    "    regression = RandomForestRegressor(random_state=random_state)\n",
    "    for distribution in distributions:\n",
    "        \n",
    "        # Implement the model fitting and evaluation logic here\n",
    "        # print('\\n')\n",
    "        # print('Distribution:', distribution.name)\n",
    "        # print('Model:', str(regression))\n",
    "\n",
    "        regression.fit(X_train, y_train)\n",
    "\n",
    "        # trained_regressors[str(regression)+'_'+str(distribution.name)]=[regression]\n",
    "\n",
    "        # getting point forecasts\n",
    "        point_forecasts = regression.predict(X_test)\n",
    "        std = y_train.std()\n",
    "        if distribution == lognorm:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, s=std) for x_ in point_forecasts])\n",
    "        elif distribution == genextreme:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, c = -0.1, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "        elif distribution != lognorm:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "        \n",
    "        #y_test_ = new_df['marked'].loc[y_test.index]\n",
    "        y_test_ = (y_test > THRESHOLD).astype(int)\n",
    "        \n",
    "        y_pred_binary = np.where(exceedance_prob >= 0.5, 1, 0)\n",
    "        # results['actual_predictions_from_model'+'_'+str(regression)+'_'+str(distribution.name)]  = point_forecasts\n",
    "        # results['y_pred_binary'+'_'+str(regression)+'_'+str(distribution.name)]  = y_pred_binary\n",
    "        # trained_regressors[str(regression)+'_'+str(distribution.name)].append(y_pred_binary)\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(y_test_, y_pred_binary)\n",
    "\n",
    "        # Compute the precision, recall, and F1 score\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        roc_auc = roc_auc_score(y_test_, exceedance_prob)\n",
    "\n",
    "        print('Model:', 'Gradient Boosting Cerceira',\n",
    "          'c-index:', None,\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', roc_auc)\n",
    "    \n",
    "        results.append([name, \n",
    "                        'Gradient_Boosting_Cerceira', \n",
    "                        None,\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        None, \n",
    "                        distribution])\n",
    "\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ###########################################################################################################################################################\n",
    "    ## Standard Binary Classification (Baseline)\n",
    "    rf_classifier = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    #THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "    y_train_binary = (y_train > THRESHOLD).astype(int)\n",
    "    rf_classifier.fit(X_train, y_train_binary)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_, y_pred)\n",
    "    precision = precision_score(y_test_, y_pred)\n",
    "    recall = recall_score(y_test_, y_pred)\n",
    "    f1 = f1_score(y_test_, y_pred)\n",
    "\n",
    "    print('Model:', 'Gradient_Boosting Binary Clf',\n",
    "          'c-index:', None,\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', None)\n",
    "    \n",
    "    results.append([name, \n",
    "                    'Gradient_Boosting_CLF', \n",
    "                    None,\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    None, \n",
    "                    None])\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest, ExtraSurvivalTrees\n",
    "from sksurv.tree import SurvivalTree\n",
    "from sksurv.svm import FastKernelSurvivalSVM, FastSurvivalSVM, NaiveSurvivalSVM\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "import pickle \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from scipy.stats import norm, laplace, logistic, gumbel_r, lognorm, cauchy, genextreme \n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "scores_RandomSurvivalForest = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "distributions = [genextreme, norm, laplace, logistic, gumbel_r, lognorm, cauchy]\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "# Loop through models and distributions and evaluate each one\n",
    "for name, values in splitted_data.items():\n",
    "    \n",
    "    X_train_survival = values['X_train_survival']\n",
    "    X_test_survival = values['X_test_survival']\n",
    "    y_train_survival = values['y_train_survival']\n",
    "    y_test_survival = values['y_test_survival']\n",
    "\n",
    "    X_train = values['X_train']\n",
    "    y_train = values['y_train']\n",
    "    X_test = values['X_test']\n",
    "    y_test = values['y_test']\n",
    "\n",
    "    X_train_mn_ext_feat = values['X_train_mn_ext_feat']\n",
    "    X_test_mn_ext_feat = values['X_test_mn_ext_feat']\n",
    "    y_train_mn_ext_feat = values['y_train_mn_ext_feat'] \n",
    "    y_test_mn_ext_feat = values['y_test_mn_ext_feat']\n",
    "    \n",
    "    X_train_tsfel = values['X_train_tsfel']\n",
    "    y_train_tsfel = values['y_train_tsfel']\n",
    "    X_test_tsfel = values['X_test_tsfel']\n",
    "    y_test_tsfel = values['y_test_tsfel']\n",
    "\n",
    "    # THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "    THRESHOLD = np.quantile(y_train, 0.85)\n",
    "\n",
    "    ## Risk Prediction approach \n",
    "    est_cph_tree = NaiveSurvivalSVM(random_state=random_state)\n",
    "    \n",
    "    est_cph_tree.fit(X_train_survival, y_train_survival)\n",
    "    # print('c-index:', est_cph_tree.score(X_test_survival, y_test_survival))\n",
    "\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "    \n",
    "    for i in range(5, 61):\n",
    "\n",
    "        #### New\n",
    "        historical_predicted_risks = est_cph_tree.predict(X_train_survival)\n",
    "\n",
    "        y_train_ = (y_train_survival['extreme_event'] == True).astype(int)\n",
    "        \n",
    "        for key, value in enumerate(y_train_):\n",
    "            if value ==1: \n",
    "                guide_n = key\n",
    "                break\n",
    "\n",
    "        guide = historical_predicted_risks[guide_n]\n",
    "\n",
    "        #### Old\n",
    "        predicted_risks = est_cph_tree.predict(X_test_survival)\n",
    "\n",
    "        # for key, value in enumerate(y_test_):\n",
    "        #     if value ==1: \n",
    "        #         guide_n = key\n",
    "        #         break\n",
    "\n",
    "        # guide = predicted_risks[guide_n]\n",
    "\n",
    "        y_pred_binary = []\n",
    "\n",
    "        for key, value in enumerate(predicted_risks):\n",
    "            if value > guide:\n",
    "                y_pred_binary.append(1)\n",
    "            elif value < guide:\n",
    "                y_pred_binary.append(0)\n",
    "            else: \n",
    "                y_pred_binary.append(1)\n",
    "\n",
    "        predictions[str(est_cph_tree)+'_risk-prediction'] = y_pred_binary\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        #roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "\n",
    "\n",
    "        print('Model:', str(est_cph_tree)+'_risk-prediction', \n",
    "            'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "            'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', None\n",
    "                )\n",
    "        \n",
    "        results.append([name, \n",
    "                        str(est_cph_tree)+'_risk-prediction', \n",
    "                        np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        None, \n",
    "                        None])\n",
    "\n",
    "    # print('\\n')\n",
    "\n",
    "    # ## Survival Probability approach\n",
    "    # prob_percentages = [0.85, 0.90]\n",
    "\n",
    "\n",
    "    # for prob_percentage in prob_percentages:\n",
    "    #     list_of_lists = est_cph_tree.predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "    #     y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "    #     #Approach 1\n",
    "    #     y_pred_binary = pd.DataFrame(list_of_lists[0:,0], columns=['probabilities'])['probabilities'].apply(lambda x: 1 if x < prob_percentage else 0).values\n",
    "\n",
    "    #     # Calculate metrics\n",
    "    #     accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "    #     precision = precision_score(y_test_, y_pred_binary)\n",
    "    #     recall = recall_score(y_test_, y_pred_binary)\n",
    "    #     f1 = f1_score(y_test_, y_pred_binary)\n",
    "    #     roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "    #     print('Model:', 'scores_cph_tree'+str(i)+'_survival-prob',\n",
    "    #         'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "    #         'accuracy:', accuracy, \n",
    "    #         'precision:', precision, \n",
    "    #         'recall:', recall, \n",
    "    #         'f1-score:', f1, \n",
    "    #         'roc_auc:', roc_auc)\n",
    "        \n",
    "    #     results.append([name, \n",
    "    #                 'scores_cph_tree'+str(i)+'_survival-prob', \n",
    "    #                 np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "    #                 accuracy, \n",
    "    #                 precision, \n",
    "    #                 recall, \n",
    "    #                 f1, \n",
    "    #                 roc_auc, \n",
    "    #                 None])\n",
    "        \n",
    "    #     predictions['scores_cph_tree'+str(i)+'_survival-prob'] = y_pred_binary\n",
    "            \n",
    "\n",
    "    # print('\\n')\n",
    "\n",
    "    # for i in range(5, 61, 5):\n",
    "    #     historical_list_of_lists = est_cph_tree.predict_survival_function(X_train_survival, return_array=True)\n",
    "    #     list_of_lists = est_cph_tree.predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "    #     y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "    #     y_train_ = (y_train_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "    #     #Approach 2\n",
    "    #     predicted_mean_survival_prob = []\n",
    "    #     historical_predicted_mean_survival_prob = []\n",
    "\n",
    "    #     for prob in list_of_lists:\n",
    "    #         predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "    #     for prob_ in historical_list_of_lists:\n",
    "    #         historical_predicted_mean_survival_prob.append(np.mean(prob_))\n",
    "\n",
    "\n",
    "    #     for key, value in enumerate(y_train_):\n",
    "    #         if value ==1: \n",
    "    #             guide_k = key\n",
    "    #             break\n",
    "\n",
    "    #     guide_ = historical_predicted_mean_survival_prob[guide_k]\n",
    "\n",
    "    #     y_pred_binary_ = []\n",
    "\n",
    "    #     for key, value in enumerate(predicted_mean_survival_prob):\n",
    "    #         if value > guide_:\n",
    "    #             y_pred_binary_.append(1)\n",
    "    #         elif value < guide_:\n",
    "    #             y_pred_binary_.append(0)\n",
    "    #         else: \n",
    "    #             y_pred_binary_.append(1)\n",
    "        \n",
    "\n",
    "    #     # Calculate metrics\n",
    "    #     accuracy = accuracy_score(y_test_, y_pred_binary_)\n",
    "    #     precision = precision_score(y_test_, y_pred_binary_)\n",
    "    #     recall = recall_score(y_test_, y_pred_binary_)\n",
    "    #     f1 = f1_score(y_test_, y_pred_binary_)\n",
    "    #     #roc_auc = roc_auc_score(y_test_, list_of_lists[0:,0])\n",
    "\n",
    "\n",
    "    #     predictions['scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted'] = y_pred_binary_\n",
    "\n",
    "    #     print('Model:', 'scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted',\n",
    "    #         'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "    #         'accuracy:', accuracy, \n",
    "    #         'precision:', precision, \n",
    "    #         'recall:', recall, \n",
    "    #         'f1-score:', f1, \n",
    "    #         'roc_auc:', None)\n",
    "        \n",
    "    #     results.append([name, \n",
    "    #                 'scores_cph_tree'+str(i)+'_mean-survival-prob-adjusted', \n",
    "    #                 np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "    #                 accuracy, \n",
    "    #                 precision, \n",
    "    #                 recall, \n",
    "    #                 f1, \n",
    "    #                 None, \n",
    "    #                 None])\n",
    "            \n",
    "    \n",
    "    # print('\\n')\n",
    "\n",
    "\n",
    "    # for i in range(5, 61, 5):\n",
    "    #     list_of_lists = est_cph_tree.predict_survival_function(X_test_survival, return_array=True)\n",
    "\n",
    "    #     y_test_ = (y_test_survival['extreme_event'] == True).astype(int)\n",
    "\n",
    "    #     #Approach 3\n",
    "    #     predicted_mean_survival_prob = []\n",
    "\n",
    "    #     for prob in list_of_lists:\n",
    "    #         predicted_mean_survival_prob.append(np.mean(prob))\n",
    "\n",
    "    #     y_pred_binary = (np.array(predicted_mean_survival_prob) > 0.5).astype(int)\n",
    "\n",
    "\n",
    "    #     accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "    #     precision = precision_score(y_test_, y_pred_binary)\n",
    "    #     recall = recall_score(y_test_, y_pred_binary)\n",
    "    #     f1 = f1_score(y_test_, y_pred_binary)\n",
    "    #     roc_auc = roc_auc_score(y_test_, predicted_mean_survival_prob)\n",
    "\n",
    "    #     predictions['scores_cph_tree'+str(i)+'_mean-survival-prob'] = y_pred_binary\n",
    "\n",
    "    #     print('Model:', 'scores_cph_tree'+str(i)+'_mean-survival-prob',\n",
    "    #         'c-index:', np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "    #         'accuracy:', accuracy, \n",
    "    #         'precision:', precision, \n",
    "    #         'recall:', recall, \n",
    "    #         'f1-score:', f1, \n",
    "    #         'roc_auc:', roc_auc)\n",
    "        \n",
    "        \n",
    "    #     results.append([name, \n",
    "    #                 'scores_cph_tree'+str(i)+'_mean-survival-prob', \n",
    "    #                 np.round(est_cph_tree.score(X_test_survival, y_test_survival),2),\n",
    "    #                 accuracy, \n",
    "    #                 precision, \n",
    "    #                 recall, \n",
    "    #                 f1, \n",
    "    #                 roc_auc, \n",
    "    #                 None])\n",
    "            \n",
    "\n",
    "            \n",
    "    print('\\n')\n",
    "\n",
    "    ## Vitor Cerceira's approach \n",
    "\n",
    "    regression = SVR()\n",
    "    for distribution in distributions:\n",
    "        \n",
    "        # Implement the model fitting and evaluation logic here\n",
    "        # print('\\n')\n",
    "        # print('Distribution:', distribution.name)\n",
    "        # print('Model:', str(regression))\n",
    "\n",
    "        regression.fit(X_train, y_train)\n",
    "\n",
    "        # trained_regressors[str(regression)+'_'+str(distribution.name)]=[regression]\n",
    "\n",
    "        # getting point forecasts\n",
    "        point_forecasts = regression.predict(X_test)\n",
    "        std = y_train.std()\n",
    "        if distribution == lognorm:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, s=std) for x_ in point_forecasts])\n",
    "        elif distribution == genextreme:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, c = -0.1, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "        elif distribution != lognorm:\n",
    "            exceedance_prob = np.asarray([1 - distribution.cdf(THRESHOLD, loc=x_, scale=std) for x_ in point_forecasts])\n",
    "        \n",
    "        #y_test_ = new_df['marked'].loc[y_test.index]\n",
    "        y_test_ = (y_test > THRESHOLD).astype(int)\n",
    "        \n",
    "        y_pred_binary = np.where(exceedance_prob >= 0.5, 1, 0)\n",
    "        # results['actual_predictions_from_model'+'_'+str(regression)+'_'+str(distribution.name)]  = point_forecasts\n",
    "        # results['y_pred_binary'+'_'+str(regression)+'_'+str(distribution.name)]  = y_pred_binary\n",
    "        # trained_regressors[str(regression)+'_'+str(distribution.name)].append(y_pred_binary)\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(y_test_, y_pred_binary)\n",
    "\n",
    "        # Compute the precision, recall, and F1 score\n",
    "        accuracy = accuracy_score(y_test_, y_pred_binary)\n",
    "        precision = precision_score(y_test_, y_pred_binary)\n",
    "        recall = recall_score(y_test_, y_pred_binary)\n",
    "        f1 = f1_score(y_test_, y_pred_binary)\n",
    "        roc_auc = roc_auc_score(y_test_, exceedance_prob)\n",
    "\n",
    "        print('Model:', str(regression)+str('_Cerceira'),\n",
    "          'c-index:', None,\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', roc_auc)\n",
    "    \n",
    "        results.append([name, \n",
    "                         str(regression)+str('_Cerceira'), \n",
    "                        None,\n",
    "                        accuracy, \n",
    "                        precision, \n",
    "                        recall, \n",
    "                        f1, \n",
    "                        None, \n",
    "                        distribution])\n",
    "        \n",
    "        predictions[ str(regression)+str('_Cerceira')] = y_pred_binary\n",
    "\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    ## Standard Binary Classification (Baseline)\n",
    "    rf_classifier = SVC(random_state=random_state)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    #THRESHOLD = round(dataframes[name]['threshold'].iloc[0])\n",
    "    y_train_binary = (y_train > THRESHOLD).astype(int)\n",
    "    rf_classifier.fit(X_train, y_train_binary)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_, y_pred)\n",
    "    precision = precision_score(y_test_, y_pred)\n",
    "    recall = recall_score(y_test_, y_pred)\n",
    "    f1 = f1_score(y_test_, y_pred)\n",
    "\n",
    "    print('Model:', str(regression)+str('_BinaryCLF'),\n",
    "          'c-index:', None,\n",
    "                'accuracy:', accuracy, \n",
    "                'precision:', precision, \n",
    "                'recall:', recall, \n",
    "                'f1-score:', f1, \n",
    "                'roc_auc:', None)\n",
    "    \n",
    "    results.append([name, \n",
    "                    str(regression)+str('_BinaryCLF'), \n",
    "                    None,\n",
    "                    accuracy, \n",
    "                    precision, \n",
    "                    recall, \n",
    "                    f1, \n",
    "                    None, \n",
    "                    None])\n",
    "    \n",
    "    predictions[str(regression)+str('_BinaryCLF')] = y_pred\n",
    "    \n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
